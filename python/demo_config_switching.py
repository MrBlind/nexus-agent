#!/usr/bin/env python3
"""
é»˜è®¤é…ç½®è‡ªåŠ¨åˆ‡æ¢æ¼”ç¤º
å±•ç¤ºå¦‚ä½•é€šè¿‡ä¿®æ”¹é»˜è®¤é…ç½®æ¥è‡ªåŠ¨è°ƒç”¨ä¸åŒçš„ LLM æ¥å£
"""

import os
import sys
import asyncio
import structlog

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from src.core.unified_llm_client import UnifiedLLMClient
from src.utils.config import LLMConfig, LLMProvidersConfig, OpenAIConfig, DeepSeekConfig
from src.models.request import Message, AgentExecuteRequest

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso", utc=False),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


def create_demo_config():
    """åˆ›å»ºæ¼”ç¤ºé…ç½®"""
    return LLMProvidersConfig(
        openai=OpenAIConfig(
            api_key=os.getenv("OPENAI_API_KEY", "demo-openai-key"),
            base_url="https://api.openai.com/v1"
        ),
        deepseek=DeepSeekConfig(
            api_key=os.getenv("DEEPSEEK_API_KEY", "demo-deepseek-key"),
            base_url="https://api.deepseek.com"
        )
    )


async def demo_config_switching():
    """æ¼”ç¤ºé…ç½®åˆ‡æ¢"""
    print("ğŸš€ é»˜è®¤é…ç½®è‡ªåŠ¨åˆ‡æ¢æ¼”ç¤º")
    print("ğŸ¯ å±•ç¤ºå¦‚ä½•é€šè¿‡ä¿®æ”¹é…ç½®è‡ªåŠ¨è°ƒç”¨ä¸åŒçš„ LLM æ¥å£")
    print("=" * 70)
    
    # åˆ›å»ºæä¾›å•†é…ç½®
    providers_config = create_demo_config()
    
    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    messages = [
        Message(role="user", content="è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    ]
    
    test_request = AgentExecuteRequest(
        session_id="demo-session",
        messages=messages,
        temperature=0.7,
        max_tokens=50
    )
    
    # æ¼”ç¤ºåœºæ™¯
    scenarios = [
        {
            "name": "åœºæ™¯ 1: é»˜è®¤ä½¿ç”¨ OpenAI",
            "config": LLMConfig(
                engine_type="openai",  # ä¿®æ”¹è¿™é‡Œåˆ‡æ¢å¼•æ“
                model=None,  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
                providers=providers_config
            ),
            "description": "åªéœ€è®¾ç½® engine_type='openai'ï¼Œç³»ç»Ÿè‡ªåŠ¨ä½¿ç”¨ OpenAI çš„é…ç½®å’Œé»˜è®¤æ¨¡å‹"
        },
        {
            "name": "åœºæ™¯ 2: åˆ‡æ¢åˆ° DeepSeek",
            "config": LLMConfig(
                engine_type="deepseek",  # ä¿®æ”¹è¿™é‡Œåˆ‡æ¢å¼•æ“
                model=None,  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
                providers=providers_config
            ),
            "description": "åªéœ€ä¿®æ”¹ engine_type='deepseek'ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢åˆ° DeepSeek é…ç½®"
        },
        {
            "name": "åœºæ™¯ 3: ä½¿ç”¨ OpenAI çš„ç‰¹å®šæ¨¡å‹",
            "config": LLMConfig(
                engine_type="openai",
                model="gpt-3.5-turbo",  # æŒ‡å®šæ¨¡å‹
                providers=providers_config
            ),
            "description": "æŒ‡å®šä½¿ç”¨ OpenAI çš„ gpt-3.5-turbo æ¨¡å‹"
        },
        {
            "name": "åœºæ™¯ 4: ä½¿ç”¨ DeepSeek çš„ä»£ç æ¨¡å‹",
            "config": LLMConfig(
                engine_type="deepseek",
                model="deepseek-coder",  # æŒ‡å®šæ¨¡å‹
                providers=providers_config
            ),
            "description": "æŒ‡å®šä½¿ç”¨ DeepSeek çš„ä»£ç ä¸“ç”¨æ¨¡å‹"
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\n{i}ï¸âƒ£ {scenario['name']}")
        print(f"   ğŸ“ {scenario['description']}")
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯ - ç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®é…ç½®é€‰æ‹©å¼•æ“
            client = UnifiedLLMClient(scenario['config'])
            
            print(f"   ğŸ”§ è‡ªåŠ¨é€‰æ‹©: {client.get_engine_type()} å¼•æ“")
            print(f"   ğŸ¯ ä½¿ç”¨æ¨¡å‹: {client.get_model()}")
            
            # æ¨¡æ‹Ÿ API è°ƒç”¨ï¼ˆå®é™…ç¯å¢ƒä¸­ä¼šå‘é€çœŸå®è¯·æ±‚ï¼‰
            print(f"   ğŸ“¡ æ¨¡æ‹Ÿè°ƒç”¨: {client.get_engine_type()} API")
            print(f"   âœ… é…ç½®è‡ªåŠ¨åˆ‡æ¢æˆåŠŸ")
            
        except Exception as e:
            print(f"   âŒ é…ç½®åˆ‡æ¢å¤±è´¥: {e}")


def demo_runtime_switching():
    """æ¼”ç¤ºè¿è¡Œæ—¶åˆ‡æ¢"""
    print(f"\n{'='*20} è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢æ¼”ç¤º {'='*20}")
    
    providers_config = create_demo_config()
    
    # åˆ›å»ºåˆå§‹é…ç½®
    config = LLMConfig(
        engine_type="openai",
        model="gpt-4",
        providers=providers_config
    )
    
    try:
        client = UnifiedLLMClient(config)
        
        print(f"ğŸ”„ è¿è¡Œæ—¶åˆ‡æ¢æ¼”ç¤º:")
        print(f"   1ï¸âƒ£ åˆå§‹çŠ¶æ€: {client.get_engine_type()} - {client.get_model()}")
        
        # è¿è¡Œæ—¶åˆ‡æ¢åˆ° DeepSeek
        client.switch_engine("deepseek")
        print(f"   2ï¸âƒ£ åˆ‡æ¢å¼•æ“: {client.get_engine_type()} - {client.get_model()}")
        
        # åˆ‡æ¢æ¨¡å‹
        client.switch_engine("deepseek", model="deepseek-coder")
        print(f"   3ï¸âƒ£ åˆ‡æ¢æ¨¡å‹: {client.get_engine_type()} - {client.get_model()}")
        
        # åˆ‡æ¢å› OpenAI
        client.switch_engine("openai", model="gpt-3.5-turbo")
        print(f"   4ï¸âƒ£ åˆ‡æ¢å›æ¥: {client.get_engine_type()} - {client.get_model()}")
        
        print("   âœ… è¿è¡Œæ—¶åˆ‡æ¢æ¼”ç¤ºå®Œæˆ")
        
    except Exception as e:
        print(f"   âŒ è¿è¡Œæ—¶åˆ‡æ¢å¤±è´¥: {e}")


def demo_provider_availability():
    """æ¼”ç¤ºæä¾›å•†å¯ç”¨æ€§æ£€æŸ¥"""
    print(f"\n{'='*20} æä¾›å•†å¯ç”¨æ€§æ£€æŸ¥æ¼”ç¤º {'='*20}")
    
    # åˆ›å»ºéƒ¨åˆ†é…ç½®çš„æä¾›å•†ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
    providers_config = LLMProvidersConfig(
        openai=OpenAIConfig(
            api_key=os.getenv("OPENAI_API_KEY", ""),  # å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰
            base_url="https://api.openai.com/v1"
        ),
        deepseek=DeepSeekConfig(
            api_key=os.getenv("DEEPSEEK_API_KEY", ""),  # å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰
            base_url="https://api.deepseek.com"
        )
    )
    
    config = LLMConfig(
        engine_type="openai",
        providers=providers_config
    )
    
    try:
        client = UnifiedLLMClient(config)
        available = client.get_available_providers()
        
        print("ğŸ” æä¾›å•†å¯ç”¨æ€§æ£€æŸ¥:")
        for provider, is_available in available.items():
            if ModelFactory.is_engine_implemented(provider):
                status = "âœ… å¯ç”¨" if is_available else "âŒ ç¼ºå°‘å¯†é’¥"
                impl_status = "å·²å®ç°"
            else:
                status = "â³ è®¡åˆ’ä¸­"
                impl_status = "æœªå®ç°"
            
            print(f"   â€¢ {provider}: {status} ({impl_status})")
        
        # æ˜¾ç¤ºå»ºè®®
        available_engines = [p for p, avail in available.items() if avail and ModelFactory.is_engine_implemented(p)]
        if available_engines:
            print(f"\nğŸ’¡ å»ºè®®ä½¿ç”¨: {', '.join(available_engines)}")
        else:
            print(f"\nâš ï¸  è¯·é…ç½®è‡³å°‘ä¸€ä¸ªæä¾›å•†çš„ API å¯†é’¥")
        
    except Exception as e:
        print(f"âŒ å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {e}")


def show_configuration_examples():
    """æ˜¾ç¤ºé…ç½®ç¤ºä¾‹"""
    print(f"\n{'='*20} é…ç½®ç¤ºä¾‹ {'='*20}")
    
    print("ğŸ”§ ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹:")
    print("""
# æ–¹æ³• 1: é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
export LLM_ENGINE_TYPE=openai          # é»˜è®¤å¼•æ“
export LLM_MODEL=gpt-4                 # é»˜è®¤æ¨¡å‹
export LLM_PROVIDERS_OPENAI_API_KEY=your_openai_key
export LLM_PROVIDERS_DEEPSEEK_API_KEY=your_deepseek_key

# æ–¹æ³• 2: é€šè¿‡ .env æ–‡ä»¶é…ç½®
# åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®:
LLM_ENGINE_TYPE=deepseek               # åˆ‡æ¢åˆ° DeepSeek
LLM_MODEL=deepseek-chat               # ä½¿ç”¨å¯¹è¯æ¨¡å‹
""")
    
    print("ğŸ’» ä»£ç é…ç½®ç¤ºä¾‹:")
    print("""
# åˆ›å»ºé…ç½®
config = LLMConfig(
    engine_type="openai",              # åªéœ€ä¿®æ”¹è¿™é‡Œ
    model="gpt-4",                     # å¯é€‰ï¼šæŒ‡å®šæ¨¡å‹
    providers=providers_config
)

# ç³»ç»Ÿè‡ªåŠ¨æ ¹æ®é…ç½®è°ƒç”¨ç›¸åº”çš„æ¥å£
client = UnifiedLLMClient(config)     # è‡ªåŠ¨é€‰æ‹© OpenAI

# è¿è¡Œæ—¶åˆ‡æ¢
client.switch_engine("deepseek")      # åˆ‡æ¢åˆ° DeepSeek
""")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸŒŸ Nexus Agent - é»˜è®¤é…ç½®è‡ªåŠ¨åˆ‡æ¢åŠŸèƒ½æ¼”ç¤º")
    print("ğŸ“‹ åŠŸèƒ½: æ ¹æ®é…ç½®è‡ªåŠ¨è°ƒç”¨ç›¸å…³çš„ LLM æ¥å£")
    print("ğŸ¯ ä¼˜åŠ¿: ä¸€è¡Œé…ç½®ä¿®æ”¹ï¼Œè‡ªåŠ¨åˆ‡æ¢æ•´ä¸ª LLM åç«¯")
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_config_switching())
    demo_runtime_switching()
    demo_provider_availability()
    show_configuration_examples()
    
    print(f"\n{'='*70}")
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    
    print("\nâœ¨ æ ¸å¿ƒç‰¹æ€§æ€»ç»“:")
    print("   â€¢ ğŸ”§ é…ç½®é©±åŠ¨: ä¿®æ”¹é…ç½®è‡ªåŠ¨åˆ‡æ¢ LLM å¼•æ“")
    print("   â€¢ ğŸš€ é›¶ä»£ç åˆ‡æ¢: ä¸éœ€è¦ä¿®æ”¹ä¸šåŠ¡é€»è¾‘ä»£ç ")
    print("   â€¢ ğŸ”„ è¿è¡Œæ—¶åˆ‡æ¢: æ”¯æŒåŠ¨æ€åˆ‡æ¢å¼•æ“å’Œæ¨¡å‹")
    print("   â€¢ ğŸ›¡ï¸ æ™ºèƒ½éªŒè¯: è‡ªåŠ¨æ£€æŸ¥é…ç½®æœ‰æ•ˆæ€§")
    print("   â€¢ ğŸ“Š çŠ¶æ€é€æ˜: æ¸…æ™°æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„å¼•æ“å’Œæ¨¡å‹")
    
    print("\nğŸš€ ä½¿ç”¨æ­¥éª¤:")
    print("   1. é…ç½®å¤šä¸ªæä¾›å•†çš„ API å¯†é’¥")
    print("   2. è®¾ç½®é»˜è®¤å¼•æ“ç±»å‹ (LLM_ENGINE_TYPE)")
    print("   3. ç³»ç»Ÿè‡ªåŠ¨ä½¿ç”¨ç›¸åº”çš„é…ç½®å’Œæ¥å£")
    print("   4. éœ€è¦åˆ‡æ¢æ—¶åªéœ€ä¿®æ”¹é…ç½®å³å¯")
    
    print("\nğŸ’¡ å®é™…åº”ç”¨:")
    print("   â€¢ å¼€å‘ç¯å¢ƒä½¿ç”¨ä¾¿å®œçš„ DeepSeek")
    print("   â€¢ ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¨³å®šçš„ OpenAI")
    print("   â€¢ A/B æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•ˆæœ")
    print("   â€¢ æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸“ç”¨æ¨¡å‹")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ¼”ç¤ºè¢«ä¸­æ–­")
    except Exception as e:
        logger.error("æ¼”ç¤ºå¤±è´¥", error=str(e))
        print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
        
# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from src.core.model_factory import ModelFactory
